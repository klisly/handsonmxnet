{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from mxnet import autograd, nd\n",
    "from mxnet.gluon import nn\n",
    "from mxnet import gluon\n",
    "from mxnet import init\n",
    "import common as comm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1 卷积神经网络\n",
    "卷积神经网络均使用最常见的二维卷积层。它有高和宽两个空间维度，常用来处理图像数据。\n",
    "###### 二维互相关运算\n",
    "在二维互相关运算中，卷积窗口从输入数组的最左上方开始，按从左往右、从上往下的顺序，依次在输入数组上滑动。当卷积窗口滑动到某一位置时，窗口中的输入子数组与核数组按元素相乘并求和，得到输出数组中相应位置的元素"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corr2d(X, K):\n",
    "    h, w = K.shape\n",
    "    Y = nd.zeros((X.shape[0] - h + 1, X.shape[1] - w + 1))\n",
    "    for i in range(Y.shape[0]):\n",
    "        for j in range(Y.shape[1]):\n",
    "            Y[i, j] = ((X[i:i + h, j : j + w]) * K).sum()\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[[19. 25.]\n",
       " [37. 43.]]\n",
       "<NDArray 2x2 @cpu(0)>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = nd.array([[0, 1, 2], [3, 4, 5], [6, 7, 8]])\n",
    "K = nd.array([[0, 1], [2, 3]])\n",
    "corr2d(X, K)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 二维卷积层¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv2D(nn.Block):\n",
    "    def __init__(self, kernel_size, **kwargs):\n",
    "        super(Conv2D, self).__init__(**kwargs)\n",
    "        self.weight = self.params.get('weight', shape = kernel_size)\n",
    "        self.bias = self.params.get('bias', shape=(1,))\n",
    "    def forwar(self, x):\n",
    "        return corr2d(x, self.weight.data()) + self.bias.data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[[1. 1. 0. 0. 0. 0. 1. 1.]\n",
       " [1. 1. 0. 0. 0. 0. 1. 1.]\n",
       " [1. 1. 0. 0. 0. 0. 1. 1.]\n",
       " [1. 1. 0. 0. 0. 0. 1. 1.]\n",
       " [1. 1. 0. 0. 0. 0. 1. 1.]\n",
       " [1. 1. 0. 0. 0. 0. 1. 1.]]\n",
       "<NDArray 6x8 @cpu(0)>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = nd.ones((6, 8))\n",
    "X[:, 2:6] = 0\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = nd.array([[1, -1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[[ 0.  1.  0.  0.  0. -1.  0.]\n",
       " [ 0.  1.  0.  0.  0. -1.  0.]\n",
       " [ 0.  1.  0.  0.  0. -1.  0.]\n",
       " [ 0.  1.  0.  0.  0. -1.  0.]\n",
       " [ 0.  1.  0.  0.  0. -1.  0.]\n",
       " [ 0.  1.  0.  0.  0. -1.  0.]]\n",
       "<NDArray 6x7 @cpu(0)>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y = corr2d(X, K)\n",
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构造一个输出通道数为1（将在“多输入通道和多输出通道”一节介绍通道），核数组形状是(1, 2)的二\n",
    "# 维卷积层\n",
    "conv2d = nn.Conv2D(1, kernel_size = (1, 2))\n",
    "conv2d.initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.reshape(1, 1, 6, 8)\n",
    "Y = Y.reshape(1, 1, 6, 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 2, loss 4.949\n",
      "batch 4, loss 0.831\n",
      "batch 6, loss 0.140\n",
      "batch 8, loss 0.024\n",
      "batch 10, loss 0.004\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    with autograd.record():\n",
    "        Y_hat = conv2d(X)\n",
    "        l = (Y_hat - Y) ** 2\n",
    "    l.backward()\n",
    "    # 简单起见，这里忽略了偏差\n",
    "    conv2d.weight.data()[:] -= 3e-2 * conv2d.weight.grad()\n",
    "    if (i + 1) % 2 == 0:\n",
    "        print('batch %d, loss %.3f' % (i + 1, l.sum().asscalar()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[[ 0.9895    -0.9873705]]\n",
       "<NDArray 1x2 @cpu(0)>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv2d.weight.data().reshape((1, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果源变量和目标变量的context一致，as_in_context函数\n",
    "使目标变量和源变量共享源变量的内存或显存。\n",
    "MXNet可以指定用来存储和计算的设备，如使用内存的CPU或者使用显存的GPU。在默认情况下，MXNet会将数据创建在内存，然后利用CPU来计算。\n",
    "MXNet要求计算的所有输入数据都在内存或同一块显卡的显存上。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2 填充和步幅\n",
    "\n",
    "假设输入形状是 nh×nw ，卷积核窗口形状是 kh×kw ，那么输出形状将会是\n",
    "\n",
    "(nh−kh+1)×(nw−kw+1).\n",
    " \n",
    "所以卷积层的输出形状由输入形状和卷积核窗口形状决定。\n",
    "\n",
    "填充（padding）是指在输入高和宽的两侧填充元素（通常是0元素）\n",
    "\n",
    "在高的两侧一共填充 ph 行，在宽的两侧一共填充 pw 列，那么输出形状将会是\n",
    "\n",
    "(nh−kh+ph+1)×(nw−kw+pw+1),\n",
    " \n",
    "也就是说，输出的高和宽会分别增加 ph 和 pw 。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, 8)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 定义一个函数来计算卷积层。它初始化卷积层权重，并对输入和输出做相应的升维和降维\n",
    "def comp_conv2d(conv2d, X):\n",
    "    conv2d.initialize()\n",
    "    # (1, 1)代表批量大小和通道数（“多输入通道和多输出通道”一节将介绍）均为1\n",
    "    X = X.reshape((1, 1) + X.shape)\n",
    "    Y = conv2d(X)\n",
    "    return Y.reshape(Y.shape[2:])  # 排除不关心的前两维：批量和通道\n",
    "\n",
    "# 注意这里是两侧分别填充1行或列，所以在两侧一共填充2行或列\n",
    "conv2d = nn.Conv2D(1, kernel_size=3, padding=1)\n",
    "X = nd.random.uniform(shape=(8, 8))\n",
    "comp_conv2d(conv2d, X).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corr2d_multi_in(X, K):\n",
    "    return nd.add_n(*[corr2d(x, k) for x, k in zip(X, K)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[[ 56.  72.]\n",
       " [104. 120.]]\n",
       "<NDArray 2x2 @cpu(0)>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = nd.array([[[0, 1, 2], [3, 4, 5], [6, 7, 8]],\n",
    "              [[1, 2, 3], [4, 5, 6], [7, 8, 9]]])\n",
    "K = nd.array([[[0, 1], [2, 3]], [[1, 2], [3, 4]]])\n",
    "corr2d_multi_in(X, K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function stack:\n",
      "\n",
      "stack(*data, **kwargs)\n",
      "    Join a sequence of arrays along a new axis.\n",
      "    \n",
      "    The axis parameter specifies the index of the new axis in the dimensions of the\n",
      "    result. For example, if axis=0 it will be the first dimension and if axis=-1 it\n",
      "    will be the last dimension.\n",
      "    \n",
      "    Examples::\n",
      "    \n",
      "      x = [1, 2]\n",
      "      y = [3, 4]\n",
      "    \n",
      "      stack(x, y) = [[1, 2],\n",
      "                     [3, 4]]\n",
      "      stack(x, y, axis=1) = [[1, 3],\n",
      "                             [2, 4]]\n",
      "    \n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    data : NDArray[]\n",
      "        List of arrays to stack\n",
      "    axis : int, optional, default='0'\n",
      "        The axis in the result array along which the input arrays are stacked.\n",
      "    \n",
      "    out : NDArray, optional\n",
      "        The output NDArray to hold the result.\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    out : NDArray or list of NDArrays\n",
      "        The output of this function.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(nd.stack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function add_n:\n",
      "\n",
      "add_n(*args, **kwargs)\n",
      "    Adds all input arguments element-wise.\n",
      "    \n",
      "    .. math::\n",
      "       add\\_n(a_1, a_2, ..., a_n) = a_1 + a_2 + ... + a_n\n",
      "    \n",
      "    ``add_n`` is potentially more efficient than calling ``add`` by `n` times.\n",
      "    \n",
      "    The storage type of ``add_n`` output depends on storage types of inputs\n",
      "    \n",
      "    - add_n(row_sparse, row_sparse, ..) = row_sparse\n",
      "    - add_n(default, csr, default) = default\n",
      "    - add_n(any input combinations longer than 4 (>4) with at least one default type) = default\n",
      "    - otherwise, ``add_n`` falls all inputs back to default storage and generates default storage\n",
      "    \n",
      "    \n",
      "    \n",
      "    Defined in src/operator/tensor/elemwise_sum.cc:L155\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    args : NDArray[]\n",
      "        Positional input arguments\n",
      "    \n",
      "    out : NDArray, optional\n",
      "        The output NDArray to hold the result.\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    out : NDArray or list of NDArrays\n",
      "        The output of this function.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(nd.add_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corr2d_multi_in_out(X, K):\n",
    "    # 对K的第0维遍历，每次同输入X做互相关计算。所有结果使用stack函数合并在一起\n",
    "    return nd.stack(*[corr2d_multi_in(X, k) for k in K])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 2, 2, 2)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "K = nd.stack(K, K + 1, K + 2)\n",
    "K.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[[[[0. 1.]\n",
       "   [2. 3.]]\n",
       "\n",
       "  [[1. 2.]\n",
       "   [3. 4.]]]\n",
       "\n",
       "\n",
       " [[[1. 2.]\n",
       "   [3. 4.]]\n",
       "\n",
       "  [[2. 3.]\n",
       "   [4. 5.]]]\n",
       "\n",
       "\n",
       " [[[2. 3.]\n",
       "   [4. 5.]]\n",
       "\n",
       "  [[3. 4.]\n",
       "   [5. 6.]]]]\n",
       "<NDArray 3x2x2x2 @cpu(0)>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[[[ 56.  72.]\n",
       "  [104. 120.]]\n",
       "\n",
       " [[ 76. 100.]\n",
       "  [148. 172.]]\n",
       "\n",
       " [[ 96. 128.]\n",
       "  [192. 224.]]]\n",
       "<NDArray 3x2x2 @cpu(0)>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corr2d_multi_in_out(X, K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corr2d_multi_in_out_1x1(X, K):\n",
    "    c_i, h, w = X.shape\n",
    "    c_o = K.shape[0]\n",
    "    X = X.reshape((c_i, h * w))\n",
    "    K = K.reshape((c_o, c_i))\n",
    "    Y = nd.dot(K, X)  # 全连接层的矩阵乘法\n",
    "    return Y.reshape((c_o, h, w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 3, 3)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = nd.random.uniform(shape=(3, 3, 3))\n",
    "K = nd.random.uniform(shape=(2, 3, 1, 1))\n",
    "\n",
    "Y1 = corr2d_multi_in_out_1x1(X, K)\n",
    "Y2 = corr2d_multi_in_out(X, K)\n",
    "\n",
    "(Y1 - Y2).norm().asscalar() < 1e-6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.4 池化层\n",
    "\n",
    "同卷积层一样，池化层每次对输入数据的一个固定形状窗口（又称池化窗口）中的元素计算输出。不同于卷积层里计算输入和核的互相关性，池化层直接计算池化窗口内元素的最大值或者平均值。该运算也分别叫做最大池化或平均池化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pool2d(X, pool_size, mode = 'max'):\n",
    "    p_h, p_w = pool_size\n",
    "    Y = nd.zeros((X.shape[0] - p_h + 1, X.shape[1] - p_w + 1))\n",
    "    for i in range(Y.shape[0]):\n",
    "        for j in range(Y.shape[1]):\n",
    "            if mode == 'max':\n",
    "                Y[i, j] = X[i:i+p_h, j:j+p_w].max()\n",
    "            elif mode == 'avg':\n",
    "                Y[i, j] = X[i:i+p_h, j:j+p_w].avg()\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[[4. 5.]\n",
       " [7. 8.]]\n",
       "<NDArray 2x2 @cpu(0)>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = nd.array([[0, 1, 2], [3, 4, 5], [6, 7, 8]])\n",
    "pool2d(X, (2, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class MaxPool2D in module mxnet.gluon.nn.conv_layers:\n",
      "\n",
      "class MaxPool2D(_Pooling)\n",
      " |  Max pooling operation for two dimensional (spatial) data.\n",
      " |  \n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  pool_size: int or list/tuple of 2 ints,\n",
      " |      Size of the max pooling windows.\n",
      " |  strides: int, list/tuple of 2 ints, or None.\n",
      " |      Factor by which to downscale. E.g. 2 will halve the input size.\n",
      " |      If `None`, it will default to `pool_size`.\n",
      " |  padding: int or list/tuple of 2 ints,\n",
      " |      If padding is non-zero, then the input is implicitly\n",
      " |      zero-padded on both sides for padding number of points.\n",
      " |  layout : str, default 'NCHW'\n",
      " |      Dimension ordering of data and out ('NCHW' or 'NHWC').\n",
      " |      'N', 'C', 'H', 'W' stands for batch, channel, height, and width\n",
      " |      dimensions respectively. padding is applied on 'H' and 'W' dimension.\n",
      " |  ceil_mode : bool, default False\n",
      " |      When `True`, will use ceil instead of floor to compute the output shape.\n",
      " |  \n",
      " |  \n",
      " |  Inputs:\n",
      " |      - **data**: 4D input tensor with shape\n",
      " |        `(batch_size, in_channels, height, width)` when `layout` is `NCHW`.\n",
      " |        For other layouts shape is permuted accordingly.\n",
      " |  \n",
      " |  Outputs:\n",
      " |      - **out**: 4D output tensor with shape\n",
      " |        `(batch_size, channels, out_height, out_width)` when `layout` is `NCHW`.\n",
      " |        out_height and out_width are calculated as::\n",
      " |  \n",
      " |            out_height = floor((height+2*padding[0]-pool_size[0])/strides[0])+1\n",
      " |            out_width = floor((width+2*padding[1]-pool_size[1])/strides[1])+1\n",
      " |  \n",
      " |        When `ceil_mode` is `True`, ceil will be used instead of floor in this\n",
      " |        equation.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      MaxPool2D\n",
      " |      _Pooling\n",
      " |      mxnet.gluon.block.HybridBlock\n",
      " |      mxnet.gluon.block.Block\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, pool_size=(2, 2), strides=None, padding=0, layout='NCHW', ceil_mode=False, **kwargs)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from _Pooling:\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  hybrid_forward(self, F, x)\n",
      " |      Overrides to construct symbolic graph for this `Block`.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      x : Symbol or NDArray\n",
      " |          The first input tensor.\n",
      " |      *args : list of Symbol or list of NDArray\n",
      " |          Additional input tensors.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from mxnet.gluon.block.HybridBlock:\n",
      " |  \n",
      " |  __setattr__(self, name, value)\n",
      " |      Registers parameters.\n",
      " |  \n",
      " |  cast(self, dtype)\n",
      " |      Cast this Block to use another data type.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      dtype : str or numpy.dtype\n",
      " |          The new data type.\n",
      " |  \n",
      " |  export(self, path, epoch=0, remove_amp_cast=True)\n",
      " |      Export HybridBlock to json format that can be loaded by\n",
      " |      `SymbolBlock.imports`, `mxnet.mod.Module` or the C++ interface.\n",
      " |      \n",
      " |      .. note:: When there are only one input, it will have name `data`. When there\n",
      " |                Are more than one inputs, they will be named as `data0`, `data1`, etc.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      path : str\n",
      " |          Path to save model. Two files `path-symbol.json` and `path-xxxx.params`\n",
      " |          will be created, where xxxx is the 4 digits epoch number.\n",
      " |      epoch : int\n",
      " |          Epoch number of saved model.\n",
      " |  \n",
      " |  forward(self, x, *args)\n",
      " |      Defines the forward computation. Arguments can be either\n",
      " |      :py:class:`NDArray` or :py:class:`Symbol`.\n",
      " |  \n",
      " |  hybridize(self, active=True, **kwargs)\n",
      " |      Activates or deactivates :py:class:`HybridBlock` s recursively. Has no effect on\n",
      " |      non-hybrid children.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      active : bool, default True\n",
      " |          Whether to turn hybrid on or off.\n",
      " |      static_alloc : bool, default False\n",
      " |          Statically allocate memory to improve speed. Memory usage may increase.\n",
      " |      static_shape : bool, default False\n",
      " |          Optimize for invariant input shapes between iterations. Must also\n",
      " |          set static_alloc to True. Change of input shapes is still allowed\n",
      " |          but slower.\n",
      " |  \n",
      " |  infer_shape(self, *args)\n",
      " |      Infers shape of Parameters from inputs.\n",
      " |  \n",
      " |  infer_type(self, *args)\n",
      " |      Infers data type of Parameters from inputs.\n",
      " |  \n",
      " |  register_child(self, block, name=None)\n",
      " |      Registers block as a child of self. :py:class:`Block` s assigned to self as\n",
      " |      attributes will be registered automatically.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from mxnet.gluon.block.Block:\n",
      " |  \n",
      " |  __call__(self, *args)\n",
      " |      Calls forward. Only accepts positional arguments.\n",
      " |  \n",
      " |  apply(self, fn)\n",
      " |      Applies ``fn`` recursively to every child block as well as self.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      fn : callable\n",
      " |          Function to be applied to each submodule, of form `fn(block)`.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      this block\n",
      " |  \n",
      " |  collect_params(self, select=None)\n",
      " |      Returns a :py:class:`ParameterDict` containing this :py:class:`Block` and all of its\n",
      " |      children's Parameters(default), also can returns the select :py:class:`ParameterDict`\n",
      " |      which match some given regular expressions.\n",
      " |      \n",
      " |      For example, collect the specified parameters in ['conv1_weight', 'conv1_bias', 'fc_weight',\n",
      " |      'fc_bias']::\n",
      " |      \n",
      " |          model.collect_params('conv1_weight|conv1_bias|fc_weight|fc_bias')\n",
      " |      \n",
      " |      or collect all parameters whose names end with 'weight' or 'bias', this can be done\n",
      " |      using regular expressions::\n",
      " |      \n",
      " |          model.collect_params('.*weight|.*bias')\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      select : str\n",
      " |          regular expressions\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      The selected :py:class:`ParameterDict`\n",
      " |  \n",
      " |  initialize(self, init=<mxnet.initializer.Uniform object at 0x7f2cd77012e8>, ctx=None, verbose=False, force_reinit=False)\n",
      " |      Initializes :py:class:`Parameter` s of this :py:class:`Block` and its children.\n",
      " |      Equivalent to ``block.collect_params().initialize(...)``\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      init : Initializer\n",
      " |          Global default Initializer to be used when :py:meth:`Parameter.init` is ``None``.\n",
      " |          Otherwise, :py:meth:`Parameter.init` takes precedence.\n",
      " |      ctx : Context or list of Context\n",
      " |          Keeps a copy of Parameters on one or many context(s).\n",
      " |      verbose : bool, default False\n",
      " |          Whether to verbosely print out details on initialization.\n",
      " |      force_reinit : bool, default False\n",
      " |          Whether to force re-initialization if parameter is already initialized.\n",
      " |  \n",
      " |  load_parameters(self, filename, ctx=None, allow_missing=False, ignore_extra=False, cast_dtype=False, dtype_source='current')\n",
      " |      Load parameters from file previously saved by `save_parameters`.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      filename : str\n",
      " |          Path to parameter file.\n",
      " |      ctx : Context or list of Context, default cpu()\n",
      " |          Context(s) to initialize loaded parameters on.\n",
      " |      allow_missing : bool, default False\n",
      " |          Whether to silently skip loading parameters not represents in the file.\n",
      " |      ignore_extra : bool, default False\n",
      " |          Whether to silently ignore parameters from the file that are not\n",
      " |          present in this Block.\n",
      " |      cast_dtype : bool, default False\n",
      " |          Cast the data type of the NDArray loaded from the checkpoint to the dtype\n",
      " |          provided by the Parameter if any.\n",
      " |      dtype_source : str, default 'current'\n",
      " |          must be in {'current', 'saved'}\n",
      " |          Only valid if cast_dtype=True, specify the source of the dtype for casting\n",
      " |          the parameters\n",
      " |      References\n",
      " |      ----------\n",
      " |      `Saving and Loading Gluon Models         <https://mxnet.incubator.apache.org/tutorials/gluon/save_load_params.html>`_\n",
      " |  \n",
      " |  load_params(self, filename, ctx=None, allow_missing=False, ignore_extra=False)\n",
      " |      [Deprecated] Please use load_parameters.\n",
      " |      \n",
      " |      Load parameters from file.\n",
      " |      \n",
      " |      filename : str\n",
      " |          Path to parameter file.\n",
      " |      ctx : Context or list of Context, default cpu()\n",
      " |          Context(s) to initialize loaded parameters on.\n",
      " |      allow_missing : bool, default False\n",
      " |          Whether to silently skip loading parameters not represents in the file.\n",
      " |      ignore_extra : bool, default False\n",
      " |          Whether to silently ignore parameters from the file that are not\n",
      " |          present in this Block.\n",
      " |  \n",
      " |  name_scope(self)\n",
      " |      Returns a name space object managing a child :py:class:`Block` and parameter\n",
      " |      names. Should be used within a ``with`` statement::\n",
      " |      \n",
      " |          with self.name_scope():\n",
      " |              self.dense = nn.Dense(20)\n",
      " |      \n",
      " |      Please refer to\n",
      " |      `naming tutorial <http://mxnet.incubator.apache.org/tutorials/gluon/naming.html>`_\n",
      " |      for more info on prefix and naming.\n",
      " |  \n",
      " |  register_forward_hook(self, hook)\n",
      " |      Registers a forward hook on the block.\n",
      " |      \n",
      " |      The hook function is called immediately after :func:`forward`.\n",
      " |      It should not modify the input or output.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      hook : callable\n",
      " |          The forward hook function of form `hook(block, input, output) -> None`.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`mxnet.gluon.utils.HookHandle`\n",
      " |  \n",
      " |  register_forward_pre_hook(self, hook)\n",
      " |      Registers a forward pre-hook on the block.\n",
      " |      \n",
      " |      The hook function is called immediately before :func:`forward`.\n",
      " |      It should not modify the input or output.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      hook : callable\n",
      " |          The forward hook function of form `hook(block, input) -> None`.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`mxnet.gluon.utils.HookHandle`\n",
      " |  \n",
      " |  save_parameters(self, filename)\n",
      " |      Save parameters to file.\n",
      " |      \n",
      " |      Saved parameters can only be loaded with `load_parameters`. Note that this\n",
      " |      method only saves parameters, not model structure. If you want to save\n",
      " |      model structures, please use :py:meth:`HybridBlock.export`.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      filename : str\n",
      " |          Path to file.\n",
      " |      \n",
      " |      References\n",
      " |      ----------\n",
      " |      `Saving and Loading Gluon Models         <https://mxnet.incubator.apache.org/tutorials/gluon/save_load_params.html>`_\n",
      " |  \n",
      " |  save_params(self, filename)\n",
      " |      [Deprecated] Please use save_parameters. Note that if you want load\n",
      " |      from SymbolBlock later, please use export instead.\n",
      " |      \n",
      " |      Save parameters to file.\n",
      " |      \n",
      " |      filename : str\n",
      " |          Path to file.\n",
      " |  \n",
      " |  summary(self, *inputs)\n",
      " |      Print the summary of the model's output and parameters.\n",
      " |      \n",
      " |      The network must have been initialized, and must not have been hybridized.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      inputs : object\n",
      " |          Any input that the model supports. For any tensor in the input, only\n",
      " |          :class:`mxnet.ndarray.NDArray` is supported.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from mxnet.gluon.block.Block:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  name\n",
      " |      Name of this :py:class:`Block`, without '_' in the end.\n",
      " |  \n",
      " |  params\n",
      " |      Returns this :py:class:`Block`'s parameter dictionary (does not include its\n",
      " |      children's parameters).\n",
      " |  \n",
      " |  prefix\n",
      " |      Prefix of this :py:class:`Block`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(nn.MaxPool2D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = nd.arange(16).reshape((1, 1, 4, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[[[[ 5.  7.]\n",
       "   [13. 15.]]]]\n",
       "<NDArray 1x1x2x2 @cpu(0)>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pool2d = nn.MaxPool2D(3, padding=1, strides=2)\n",
    "pool2d(X) # 因为池化层没有模型参数，所以不需要调用参数初始化函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.5 LeNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = nn.Sequential()\n",
    "net.add(nn.Conv2D(channels=16, kernel_size=4, activation='sigmoid'),\n",
    "        nn.MaxPool2D(pool_size=2, strides=2),\n",
    "        nn.Conv2D(channels=32, kernel_size=4, activation='sigmoid'),\n",
    "        nn.MaxPool2D(pool_size=2, strides=2),\n",
    "        # Dense会默认将(批量大小, 通道, 高, 宽)形状的输入转换成\n",
    "        # (批量大小, 通道 * 高 * 宽)形状的输入\n",
    "        nn.Dense(120, activation='sigmoid'),\n",
    "        nn.Dense(84, activation='sigmoid'),\n",
    "        nn.Dense(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv2 output shape:\t (1, 16, 25, 25)\n",
      "pool1 output shape:\t (1, 16, 12, 12)\n",
      "conv3 output shape:\t (1, 32, 9, 9)\n",
      "pool2 output shape:\t (1, 32, 4, 4)\n",
      "dense0 output shape:\t (1, 120)\n",
      "dense1 output shape:\t (1, 84)\n",
      "dense2 output shape:\t (1, 10)\n"
     ]
    }
   ],
   "source": [
    "X = nd.random.uniform(shape=(1, 1, 28, 28))\n",
    "net.initialize()\n",
    "for layer in net:\n",
    "    X = layer(X)\n",
    "    print(layer.name, 'output shape:\\t', X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "train_iter, test_iter = comm.load_data_fashion_mnist(batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mxnet as mx\n",
    "from mxnet.gluon import loss as gloss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gpu(0)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " def try_gpu():\n",
    "    try:\n",
    "        ctx = mx.gpu()\n",
    "        _ = nd.zeros((1,), ctx=ctx)\n",
    "    except mx.base.MXNetError:\n",
    "        ctx = mx.cpu()\n",
    "    return ctx\n",
    "\n",
    "ctx = try_gpu()\n",
    "ctx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_accuracy(data_iter, net, ctx):\n",
    "    acc_sum, n = nd.array([0], ctx=ctx), 0\n",
    "    for X, y in data_iter:\n",
    "        # 如果ctx代表GPU及相应的显存，将数据复制到显存上\n",
    "        X, y = X.as_in_context(ctx), y.as_in_context(ctx).astype('float32')\n",
    "        acc_sum += (net(X).argmax(axis=1) == y).sum()\n",
    "        n += y.size\n",
    "    return acc_sum.asscalar() / n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ch5(net, train_iter, test_iter, batch_size, trainer, ctx,\n",
    "              num_epochs):\n",
    "    print('training on', ctx)\n",
    "    loss = gloss.SoftmaxCrossEntropyLoss()\n",
    "    for epoch in range(num_epochs):\n",
    "        train_l_sum, train_acc_sum, n, start = 0.0, 0.0, 0, time.time()\n",
    "        for X, y in train_iter:\n",
    "            X, y = X.as_in_context(ctx), y.as_in_context(ctx)\n",
    "            with autograd.record():\n",
    "                y_hat = net(X)\n",
    "                l = loss(y_hat, y).sum()\n",
    "            l.backward()\n",
    "            trainer.step(batch_size)\n",
    "            y = y.astype('float32')\n",
    "            train_l_sum += l.asscalar()\n",
    "            train_acc_sum += (y_hat.argmax(axis=1) == y).sum().asscalar()\n",
    "            n += y.size\n",
    "        test_acc = evaluate_accuracy(test_iter, net, ctx)\n",
    "        print('epoch %d, loss %.4f, train acc %.3f, test acc %.3f, '\n",
    "              'time %.1f sec'\n",
    "              % (epoch + 1, train_l_sum / n, train_acc_sum / n, test_acc,\n",
    "                 time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on gpu(0)\n",
      "epoch 1, loss 2.3195, train acc 0.101, test acc 0.100, time 2.2 sec\n",
      "epoch 2, loss 1.7528, train acc 0.338, test acc 0.583, time 2.3 sec\n",
      "epoch 3, loss 0.9802, train acc 0.625, test acc 0.671, time 2.2 sec\n",
      "epoch 4, loss 0.8344, train acc 0.683, test acc 0.703, time 2.1 sec\n",
      "epoch 5, loss 0.7425, train acc 0.715, test acc 0.737, time 2.1 sec\n",
      "epoch 6, loss 0.6924, train acc 0.731, test acc 0.744, time 2.2 sec\n",
      "epoch 7, loss 0.6553, train acc 0.745, test acc 0.750, time 2.2 sec\n",
      "epoch 8, loss 0.6275, train acc 0.756, test acc 0.769, time 2.2 sec\n",
      "epoch 9, loss 0.5978, train acc 0.767, test acc 0.785, time 2.2 sec\n",
      "epoch 10, loss 0.5734, train acc 0.778, test acc 0.794, time 2.1 sec\n",
      "epoch 11, loss 0.5493, train acc 0.788, test acc 0.801, time 2.1 sec\n",
      "epoch 12, loss 0.5327, train acc 0.794, test acc 0.807, time 2.1 sec\n",
      "epoch 13, loss 0.5126, train acc 0.802, test acc 0.812, time 2.2 sec\n",
      "epoch 14, loss 0.4957, train acc 0.810, test acc 0.823, time 2.1 sec\n",
      "epoch 15, loss 0.4851, train acc 0.815, test acc 0.827, time 2.1 sec\n",
      "epoch 16, loss 0.4732, train acc 0.821, test acc 0.830, time 2.1 sec\n",
      "epoch 17, loss 0.4615, train acc 0.826, test acc 0.834, time 2.1 sec\n",
      "epoch 18, loss 0.4467, train acc 0.833, test acc 0.832, time 2.1 sec\n",
      "epoch 19, loss 0.4408, train acc 0.835, test acc 0.841, time 2.2 sec\n",
      "epoch 20, loss 0.4329, train acc 0.838, test acc 0.848, time 2.2 sec\n",
      "epoch 21, loss 0.4219, train acc 0.841, test acc 0.847, time 2.2 sec\n",
      "epoch 22, loss 0.4156, train acc 0.845, test acc 0.836, time 2.1 sec\n",
      "epoch 23, loss 0.4087, train acc 0.847, test acc 0.851, time 2.1 sec\n",
      "epoch 24, loss 0.3989, train acc 0.851, test acc 0.851, time 2.2 sec\n",
      "epoch 25, loss 0.3965, train acc 0.851, test acc 0.852, time 2.2 sec\n",
      "epoch 26, loss 0.3916, train acc 0.853, test acc 0.857, time 2.1 sec\n",
      "epoch 27, loss 0.3828, train acc 0.856, test acc 0.861, time 2.2 sec\n",
      "epoch 28, loss 0.3783, train acc 0.857, test acc 0.862, time 2.1 sec\n",
      "epoch 29, loss 0.3749, train acc 0.860, test acc 0.859, time 2.1 sec\n",
      "epoch 30, loss 0.3671, train acc 0.861, test acc 0.852, time 2.1 sec\n",
      "epoch 31, loss 0.3662, train acc 0.863, test acc 0.864, time 2.1 sec\n",
      "epoch 32, loss 0.3580, train acc 0.866, test acc 0.870, time 2.3 sec\n",
      "epoch 33, loss 0.3526, train acc 0.869, test acc 0.869, time 2.2 sec\n",
      "epoch 34, loss 0.3509, train acc 0.868, test acc 0.869, time 2.2 sec\n",
      "epoch 35, loss 0.3450, train acc 0.871, test acc 0.858, time 2.1 sec\n",
      "epoch 36, loss 0.3399, train acc 0.874, test acc 0.859, time 2.1 sec\n",
      "epoch 37, loss 0.3355, train acc 0.874, test acc 0.874, time 2.2 sec\n",
      "epoch 38, loss 0.3346, train acc 0.875, test acc 0.875, time 2.1 sec\n",
      "epoch 39, loss 0.3313, train acc 0.875, test acc 0.877, time 2.2 sec\n",
      "epoch 40, loss 0.3266, train acc 0.878, test acc 0.874, time 2.2 sec\n",
      "epoch 41, loss 0.3238, train acc 0.879, test acc 0.878, time 2.1 sec\n",
      "epoch 42, loss 0.3198, train acc 0.879, test acc 0.876, time 2.1 sec\n",
      "epoch 43, loss 0.3195, train acc 0.880, test acc 0.881, time 2.1 sec\n",
      "epoch 44, loss 0.3152, train acc 0.882, test acc 0.885, time 2.1 sec\n",
      "epoch 45, loss 0.3132, train acc 0.883, test acc 0.880, time 2.2 sec\n",
      "epoch 46, loss 0.3085, train acc 0.885, test acc 0.882, time 2.2 sec\n",
      "epoch 47, loss 0.3063, train acc 0.886, test acc 0.878, time 2.2 sec\n",
      "epoch 48, loss 0.3047, train acc 0.888, test acc 0.883, time 2.2 sec\n",
      "epoch 49, loss 0.3019, train acc 0.887, test acc 0.880, time 2.2 sec\n",
      "epoch 50, loss 0.2982, train acc 0.889, test acc 0.885, time 2.0 sec\n",
      "epoch 51, loss 0.2957, train acc 0.891, test acc 0.881, time 2.2 sec\n",
      "epoch 52, loss 0.2922, train acc 0.890, test acc 0.888, time 2.1 sec\n",
      "epoch 53, loss 0.2930, train acc 0.891, test acc 0.887, time 2.1 sec\n",
      "epoch 54, loss 0.2890, train acc 0.892, test acc 0.880, time 2.1 sec\n",
      "epoch 55, loss 0.2875, train acc 0.892, test acc 0.882, time 2.2 sec\n",
      "epoch 56, loss 0.2825, train acc 0.895, test acc 0.886, time 2.2 sec\n",
      "epoch 57, loss 0.2822, train acc 0.895, test acc 0.891, time 2.2 sec\n",
      "epoch 58, loss 0.2817, train acc 0.894, test acc 0.894, time 2.2 sec\n",
      "epoch 59, loss 0.2775, train acc 0.896, test acc 0.892, time 2.1 sec\n",
      "epoch 60, loss 0.2759, train acc 0.897, test acc 0.885, time 2.1 sec\n",
      "epoch 61, loss 0.2749, train acc 0.897, test acc 0.892, time 2.1 sec\n",
      "epoch 62, loss 0.2704, train acc 0.899, test acc 0.892, time 2.2 sec\n",
      "epoch 63, loss 0.2689, train acc 0.900, test acc 0.896, time 2.2 sec\n",
      "epoch 64, loss 0.2683, train acc 0.900, test acc 0.892, time 2.2 sec\n",
      "epoch 65, loss 0.2669, train acc 0.901, test acc 0.893, time 2.1 sec\n",
      "epoch 66, loss 0.2655, train acc 0.900, test acc 0.893, time 2.2 sec\n",
      "epoch 67, loss 0.2650, train acc 0.902, test acc 0.896, time 2.1 sec\n",
      "epoch 68, loss 0.2581, train acc 0.904, test acc 0.896, time 2.1 sec\n",
      "epoch 69, loss 0.2596, train acc 0.903, test acc 0.892, time 2.1 sec\n",
      "epoch 70, loss 0.2553, train acc 0.905, test acc 0.891, time 2.1 sec\n",
      "epoch 71, loss 0.2573, train acc 0.904, test acc 0.898, time 2.2 sec\n",
      "epoch 72, loss 0.2551, train acc 0.904, test acc 0.895, time 2.1 sec\n",
      "epoch 73, loss 0.2501, train acc 0.907, test acc 0.896, time 2.1 sec\n",
      "epoch 74, loss 0.2509, train acc 0.906, test acc 0.898, time 2.1 sec\n",
      "epoch 75, loss 0.2482, train acc 0.906, test acc 0.897, time 2.2 sec\n",
      "epoch 76, loss 0.2468, train acc 0.907, test acc 0.900, time 2.2 sec\n",
      "epoch 77, loss 0.2447, train acc 0.908, test acc 0.898, time 2.3 sec\n",
      "epoch 78, loss 0.2433, train acc 0.909, test acc 0.901, time 2.2 sec\n",
      "epoch 79, loss 0.2422, train acc 0.910, test acc 0.902, time 2.2 sec\n",
      "epoch 80, loss 0.2403, train acc 0.911, test acc 0.890, time 2.1 sec\n",
      "epoch 81, loss 0.2388, train acc 0.911, test acc 0.900, time 2.2 sec\n",
      "epoch 82, loss 0.2387, train acc 0.911, test acc 0.901, time 2.2 sec\n",
      "epoch 83, loss 0.2360, train acc 0.911, test acc 0.901, time 2.2 sec\n",
      "epoch 84, loss 0.2347, train acc 0.912, test acc 0.900, time 2.1 sec\n",
      "epoch 85, loss 0.2333, train acc 0.912, test acc 0.895, time 2.2 sec\n",
      "epoch 86, loss 0.2321, train acc 0.913, test acc 0.899, time 2.2 sec\n",
      "epoch 87, loss 0.2303, train acc 0.913, test acc 0.901, time 2.2 sec\n",
      "epoch 88, loss 0.2287, train acc 0.914, test acc 0.897, time 2.2 sec\n",
      "epoch 89, loss 0.2266, train acc 0.914, test acc 0.900, time 2.2 sec\n",
      "epoch 90, loss 0.2272, train acc 0.915, test acc 0.901, time 2.2 sec\n",
      "epoch 91, loss 0.2214, train acc 0.916, test acc 0.902, time 2.3 sec\n",
      "epoch 92, loss 0.2219, train acc 0.917, test acc 0.903, time 2.1 sec\n",
      "epoch 93, loss 0.2229, train acc 0.916, test acc 0.903, time 2.1 sec\n",
      "epoch 94, loss 0.2195, train acc 0.917, test acc 0.900, time 2.1 sec\n",
      "epoch 95, loss 0.2159, train acc 0.920, test acc 0.901, time 2.1 sec\n",
      "epoch 96, loss 0.2191, train acc 0.917, test acc 0.905, time 2.2 sec\n",
      "epoch 97, loss 0.2144, train acc 0.919, test acc 0.906, time 2.1 sec\n",
      "epoch 98, loss 0.2128, train acc 0.920, test acc 0.907, time 2.2 sec\n",
      "epoch 99, loss 0.2137, train acc 0.919, test acc 0.906, time 2.1 sec\n",
      "epoch 100, loss 0.2123, train acc 0.921, test acc 0.901, time 2.1 sec\n"
     ]
    }
   ],
   "source": [
    "lr, num_epochs = 0.5, 100\n",
    "net.initialize(force_reinit=True, ctx=ctx, init=init.Xavier())\n",
    "trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': lr})\n",
    "train_ch5(net, train_iter, test_iter, batch_size, trainer, ctx, num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.6 AlexNet\n",
    "AlexNet与LeNet的设计理念非常相似，但也有显著的区别。\n",
    "\n",
    "第一，与相对较小的LeNet相比，AlexNet包含8层变换，其中有5层卷积和2层全连接隐藏层，以及1个全连接输出层。下面我们来详细描述这些层的设计。\n",
    "\n",
    "AlexNet第一层中的卷积窗口形状是11×11。因为ImageNet中绝大多数图像的高和宽均比MNIST图像的高和宽大10倍以上，ImageNet图像的物体占用更多的像素，所以需要更大的卷积窗口来捕获物体。第二层中的卷积窗口形状减小到5×5，之后全采用3×3。此外，第一、第二和第五个卷积层之后都使用了窗口形状为3×3、步幅为2的最大池化层。而且，AlexNet使用的卷积通道数也大于LeNet中的卷积通道数数十倍。\n",
    "\n",
    "紧接着最后一个卷积层的是两个输出个数为4096的全连接层。这两个巨大的全连接层带来将近1 GB的模型参数。由于早期显存的限制，最早的AlexNet使用双数据流的设计使一个GPU只需要处理一半模型。幸运的是，显存在过去几年得到了长足的发展，因此通常我们不再需要这样的特别设计了。\n",
    "\n",
    "第二，AlexNet将sigmoid激活函数改成了更加简单的ReLU激活函数。一方面，ReLU激活函数的计算更简单，例如它并没有sigmoid激活函数中的求幂运算。另一方面，ReLU激活函数在不同的参数初始化方法下使模型更容易训练。这是由于当sigmoid激活函数输出极接近0或1时，这些区域的梯度几乎为0，从而造成反向传播无法继续更新部分模型参数；而ReLU激活函数在正区间的梯度恒为1。因此，若模型参数初始化不当，sigmoid函数可能在正区间得到几乎为0的梯度，从而令模型无法得到有效训练。\n",
    "\n",
    "第三，AlexNet通过丢弃法（参见“丢弃法”一节）来控制全连接层的模型复杂度。而LeNet并没有使用丢弃法。\n",
    "\n",
    "第四，AlexNet引入了大量的图像增广，如翻转、裁剪和颜色变化，从而进一步扩大数据集来缓解过拟合。我们将在后面的“图像增广”一节详细介绍这种方法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "net = nn.Sequential()\n",
    "# 使用较大的11 x 11窗口来捕获物体。同时使用步幅4来较大幅度减小输出高和宽。这里使用的输出通\n",
    "# 道数比LeNet中的也要大很多\n",
    "net.add(nn.Conv2D(96, kernel_size=11, strides=4, activation='relu'),\n",
    "        nn.MaxPool2D(pool_size=3, strides=2),\n",
    "        # 减小卷积窗口，使用填充为2来使得输入与输出的高和宽一致，且增大输出通道数\n",
    "        nn.Conv2D(256, kernel_size=5, padding=2, activation='relu'),\n",
    "        nn.MaxPool2D(pool_size=3, strides=2),\n",
    "        # 连续3个卷积层，且使用更小的卷积窗口。除了最后的卷积层外，进一步增大了输出通道数。\n",
    "        # 前两个卷积层后不使用池化层来减小输入的高和宽\n",
    "        nn.Conv2D(384, kernel_size=3, padding=1, activation='relu'),\n",
    "        nn.Conv2D(384, kernel_size=3, padding=1, activation='relu'),\n",
    "        nn.Conv2D(256, kernel_size=3, padding=1, activation='relu'),\n",
    "        nn.MaxPool2D(pool_size=3, strides=2),\n",
    "        # 这里全连接层的输出个数比LeNet中的大数倍。使用丢弃层来缓解过拟合\n",
    "        nn.Dense(4096, activation=\"relu\"), nn.Dropout(0.5),\n",
    "        nn.Dense(4096, activation=\"relu\"), nn.Dropout(0.5),\n",
    "        # 输出层。由于这里使用Fashion-MNIST，所以用类别数为10，而非论文中的1000\n",
    "        nn.Dense(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv4 output shape:\t (1, 96, 54, 54)\n",
      "pool3 output shape:\t (1, 96, 26, 26)\n",
      "conv5 output shape:\t (1, 256, 26, 26)\n",
      "pool4 output shape:\t (1, 256, 12, 12)\n",
      "conv6 output shape:\t (1, 384, 12, 12)\n",
      "conv7 output shape:\t (1, 384, 12, 12)\n",
      "conv8 output shape:\t (1, 256, 12, 12)\n",
      "pool5 output shape:\t (1, 256, 5, 5)\n",
      "dense3 output shape:\t (1, 4096)\n",
      "dropout0 output shape:\t (1, 4096)\n",
      "dense4 output shape:\t (1, 4096)\n",
      "dropout1 output shape:\t (1, 4096)\n",
      "dense5 output shape:\t (1, 10)\n"
     ]
    }
   ],
   "source": [
    "X = nd.random.uniform(shape=(1, 1, 224, 224))\n",
    "net.initialize()\n",
    "for layer in net:\n",
    "    X = layer(X)\n",
    "    print(layer.name, 'output shape:\\t', X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "# 如出现“out of memory”的报错信息，可减小batch_size或resize\n",
    "train_iter, test_iter = comm.load_data_fashion_mnist(batch_size, resize=224)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on gpu(0)\n",
      "epoch 1, loss 1.3751, train acc 0.495, test acc 0.743, time 30.7 sec\n",
      "epoch 2, loss 0.6608, train acc 0.751, test acc 0.804, time 30.6 sec\n",
      "epoch 3, loss 0.5426, train acc 0.800, test acc 0.838, time 30.7 sec\n",
      "epoch 4, loss 0.4780, train acc 0.823, test acc 0.857, time 30.8 sec\n",
      "epoch 5, loss 0.4287, train acc 0.843, test acc 0.865, time 30.8 sec\n",
      "epoch 6, loss 0.3977, train acc 0.854, test acc 0.878, time 30.7 sec\n",
      "epoch 7, loss 0.3745, train acc 0.864, test acc 0.877, time 30.9 sec\n",
      "epoch 8, loss 0.3566, train acc 0.870, test acc 0.885, time 30.8 sec\n",
      "epoch 9, loss 0.3403, train acc 0.875, test acc 0.884, time 30.8 sec\n",
      "epoch 10, loss 0.3262, train acc 0.881, test acc 0.888, time 30.7 sec\n",
      "epoch 11, loss 0.3156, train acc 0.885, test acc 0.891, time 30.8 sec\n",
      "epoch 12, loss 0.3057, train acc 0.888, test acc 0.898, time 30.7 sec\n",
      "epoch 13, loss 0.2945, train acc 0.892, test acc 0.899, time 30.8 sec\n",
      "epoch 14, loss 0.2888, train acc 0.895, test acc 0.902, time 30.5 sec\n",
      "epoch 15, loss 0.2803, train acc 0.898, test acc 0.908, time 30.7 sec\n",
      "epoch 16, loss 0.2723, train acc 0.901, test acc 0.908, time 30.8 sec\n",
      "epoch 17, loss 0.2643, train acc 0.903, test acc 0.905, time 30.7 sec\n",
      "epoch 18, loss 0.2576, train acc 0.905, test acc 0.903, time 30.7 sec\n",
      "epoch 19, loss 0.2534, train acc 0.907, test acc 0.909, time 30.7 sec\n",
      "epoch 20, loss 0.2466, train acc 0.909, test acc 0.910, time 30.8 sec\n",
      "epoch 21, loss 0.2414, train acc 0.911, test acc 0.913, time 30.8 sec\n",
      "epoch 22, loss 0.2339, train acc 0.913, test acc 0.915, time 30.6 sec\n",
      "epoch 23, loss 0.2304, train acc 0.915, test acc 0.913, time 30.7 sec\n",
      "epoch 24, loss 0.2239, train acc 0.917, test acc 0.916, time 30.8 sec\n",
      "epoch 25, loss 0.2193, train acc 0.919, test acc 0.907, time 30.8 sec\n",
      "epoch 26, loss 0.2137, train acc 0.920, test acc 0.910, time 30.8 sec\n",
      "epoch 27, loss 0.2093, train acc 0.921, test acc 0.911, time 30.1 sec\n",
      "epoch 28, loss 0.2049, train acc 0.924, test acc 0.917, time 30.9 sec\n",
      "epoch 29, loss 0.1980, train acc 0.926, test acc 0.919, time 30.4 sec\n",
      "epoch 30, loss 0.1967, train acc 0.926, test acc 0.918, time 30.8 sec\n",
      "epoch 31, loss 0.1892, train acc 0.931, test acc 0.913, time 30.8 sec\n",
      "epoch 32, loss 0.1865, train acc 0.931, test acc 0.920, time 30.7 sec\n",
      "epoch 33, loss 0.1829, train acc 0.932, test acc 0.923, time 30.8 sec\n",
      "epoch 34, loss 0.1767, train acc 0.934, test acc 0.923, time 30.4 sec\n",
      "epoch 35, loss 0.1710, train acc 0.936, test acc 0.923, time 30.4 sec\n",
      "epoch 36, loss 0.1669, train acc 0.937, test acc 0.921, time 30.8 sec\n",
      "epoch 37, loss 0.1632, train acc 0.940, test acc 0.923, time 30.8 sec\n",
      "epoch 38, loss 0.1605, train acc 0.940, test acc 0.925, time 30.7 sec\n",
      "epoch 39, loss 0.1565, train acc 0.942, test acc 0.922, time 30.3 sec\n",
      "epoch 40, loss 0.1496, train acc 0.944, test acc 0.921, time 30.7 sec\n",
      "epoch 41, loss 0.1492, train acc 0.944, test acc 0.927, time 30.8 sec\n",
      "epoch 42, loss 0.1443, train acc 0.946, test acc 0.925, time 30.4 sec\n",
      "epoch 43, loss 0.1403, train acc 0.947, test acc 0.924, time 30.8 sec\n",
      "epoch 44, loss 0.1346, train acc 0.950, test acc 0.925, time 30.7 sec\n",
      "epoch 45, loss 0.1331, train acc 0.951, test acc 0.925, time 30.4 sec\n",
      "epoch 46, loss 0.1296, train acc 0.952, test acc 0.926, time 30.6 sec\n",
      "epoch 47, loss 0.1256, train acc 0.954, test acc 0.926, time 30.2 sec\n",
      "epoch 48, loss 0.1213, train acc 0.954, test acc 0.922, time 30.8 sec\n",
      "epoch 49, loss 0.1179, train acc 0.956, test acc 0.927, time 30.8 sec\n",
      "epoch 50, loss 0.1146, train acc 0.957, test acc 0.925, time 30.6 sec\n"
     ]
    }
   ],
   "source": [
    "lr, num_epochs, ctx = 0.01, 50, comm.try_gpu()\n",
    "net.initialize(force_reinit=True, ctx=ctx, init=init.Xavier())\n",
    "trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': lr})\n",
    "comm.train_ch5(net, train_iter, test_iter, batch_size, trainer, ctx, num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.6 使用重复元素的网络（VGG）\n",
    "VGG块的组成规律是：连续使用数个相同的填充为1、窗口形状为3×3的卷积层后接上一个步幅为2、窗口形状为2×2的最大池化层。卷积层保持输入的高和宽不变，而池化层则对其减半。我们使用vgg_block函数来实现这个基础的VGG块，它可以指定卷积层的数量num_convs和输出通道数num_channels。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vgg_block(num_convs, num_channels):\n",
    "    blk = nn.Sequential()\n",
    "    for _ in range(num_convs):\n",
    "        blk.add(nn.Conv2D(num_channels, kernel_size = 3, padding = 1, activation = 'relu'))\n",
    "    blk.add(nn.MaxPool2D(pool_size=2, strides = 2))\n",
    "    return blk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在我们构造一个VGG网络。它有5个卷积块，前2块使用单卷积层，而后3块使用双卷积层。\n",
    "\n",
    "第一块的输出通道是64，之后每次对输出通道数翻倍，直到变为512。\n",
    "\n",
    "因为这个网络使用了8个卷积层和3个全连接层，所以经常被称为VGG-11。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_arch = ((1, 64), (1, 128), (2, 256), (2, 512), (2, 512))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vgg(conv_arch):\n",
    "    net = nn.Sequential()\n",
    "    #卷积层部分\n",
    "    for(num_convs, num_channels) in conv_arch:\n",
    "        net.add(vgg_block(num_convs, num_channels))\n",
    "    #全连接层部分\n",
    "    net.add(nn.Dense(4096, activation = 'relu'))\n",
    "    net.add(nn.Dense(4096, activation = 'relu'))\n",
    "    net.add(nn.Dense(10))\n",
    "    return net;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = vgg(conv_arch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequential3 output shape:\t (1, 64, 112, 112)\n",
      "sequential4 output shape:\t (1, 128, 56, 56)\n",
      "sequential5 output shape:\t (1, 256, 28, 28)\n",
      "sequential6 output shape:\t (1, 512, 14, 14)\n",
      "sequential7 output shape:\t (1, 512, 7, 7)\n",
      "dense6 output shape:\t (1, 4096)\n",
      "dense7 output shape:\t (1, 4096)\n",
      "dense8 output shape:\t (1, 10)\n"
     ]
    }
   ],
   "source": [
    "net.initialize(init.Normal(sigma = 0.05))\n",
    "X = nd.random.uniform(shape=(1, 1, 224, 224))\n",
    "for blk in net:\n",
    "    X = blk(X)\n",
    "    print(blk.name, 'output shape:\\t', X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取数据并训练\n",
    "ratio = 4\n",
    "small_conv_arch = [(pair[0], pair[1] // ratio) for pair in conv_arch]\n",
    "net = vgg(small_conv_arch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on gpu(0)\n",
      "epoch 1, loss 0.8026, train acc 0.720, test acc 0.865, time 64.8 sec\n",
      "epoch 2, loss 0.3489, train acc 0.872, test acc 0.881, time 63.1 sec\n",
      "epoch 3, loss 0.2832, train acc 0.897, test acc 0.888, time 62.1 sec\n",
      "epoch 4, loss 0.2385, train acc 0.912, test acc 0.906, time 61.9 sec\n",
      "epoch 5, loss 0.2013, train acc 0.925, test acc 0.915, time 62.4 sec\n"
     ]
    }
   ],
   "source": [
    "lr, num_epochs, batch_size, ctx = 0.05, 5, 128, comm.try_gpu()\n",
    "net.initialize(ctx=ctx, init=init.Xavier())\n",
    "trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': lr})\n",
    "train_iter, test_iter = comm.load_data_fashion_mnist(batch_size, resize=224)\n",
    "comm.train_ch5(net, train_iter, test_iter, batch_size, trainer, ctx,\n",
    "              num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.8 NiN 网络中的网络\n",
    "\n",
    "NiN块是NiN中的基础块。它由一个卷积层加两个充当全连接层的 1×1 卷积层串联而成。其中第一个卷积层的超参数可以自行设置，而第二和第三个卷积层的超参数一般是固定的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nin_block(num_channels, kernel_size, strides, padding):\n",
    "    blk = nn.Sequential()\n",
    "    blk.add(nn.Conv2D(num_channels, kernel_size, strides, padding, activation = 'relu'))\n",
    "    blk.add(nn.Conv2D(num_channels, kernel_size = 1, activation='relu'))\n",
    "    blk.add(nn.Conv2D(num_channels, kernel_size = 1, activation='relu'))\n",
    "    return blk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = nn.Sequential()\n",
    "net.add(nin_block(96, kernel_size=11, strides=4, padding=0),\n",
    "        nn.MaxPool2D(pool_size=3, strides=2),\n",
    "        nin_block(256, kernel_size=5, strides=1, padding=2),\n",
    "        nn.MaxPool2D(pool_size=3, strides=2),\n",
    "        nin_block(384, kernel_size=3, strides=1, padding=1),\n",
    "        nn.MaxPool2D(pool_size=3, strides=2), nn.Dropout(0.5),\n",
    "        # 标签类别数是10\n",
    "        nin_block(10, kernel_size=3, strides=1, padding=1),\n",
    "        # 全局平均池化层将窗口形状自动设置成输入的高和宽\n",
    "        nn.GlobalAvgPool2D(),\n",
    "        # 将四维的输出转成二维的输出，其形状为(批量大小, 10)\n",
    "        nn.Flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequential25 output shape:\t (1, 96, 54, 54)\n",
      "pool24 output shape:\t (1, 96, 26, 26)\n",
      "sequential26 output shape:\t (1, 256, 26, 26)\n",
      "pool25 output shape:\t (1, 256, 12, 12)\n",
      "sequential27 output shape:\t (1, 384, 12, 12)\n",
      "pool26 output shape:\t (1, 384, 5, 5)\n",
      "dropout4 output shape:\t (1, 384, 5, 5)\n",
      "sequential28 output shape:\t (1, 10, 5, 5)\n",
      "pool27 output shape:\t (1, 10, 1, 1)\n",
      "flatten2 output shape:\t (1, 10)\n"
     ]
    }
   ],
   "source": [
    "X = nd.random.uniform(shape=(1, 1, 224, 224))\n",
    "net.initialize()\n",
    "for layer in net:\n",
    "    X = layer(X)\n",
    "    print(layer.name, 'output shape:\\t', X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Sequential(\n",
       "    (0): Conv2D(1 -> 96, kernel_size=(11, 11), stride=(4, 4), Activation(relu))\n",
       "    (1): Conv2D(96 -> 96, kernel_size=(1, 1), stride=(1, 1), Activation(relu))\n",
       "    (2): Conv2D(96 -> 96, kernel_size=(1, 1), stride=(1, 1), Activation(relu))\n",
       "  )\n",
       "  (1): MaxPool2D(size=(3, 3), stride=(2, 2), padding=(0, 0), ceil_mode=False, global_pool=False, pool_type=max, layout=NCHW)\n",
       "  (2): Sequential(\n",
       "    (0): Conv2D(96 -> 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), Activation(relu))\n",
       "    (1): Conv2D(256 -> 256, kernel_size=(1, 1), stride=(1, 1), Activation(relu))\n",
       "    (2): Conv2D(256 -> 256, kernel_size=(1, 1), stride=(1, 1), Activation(relu))\n",
       "  )\n",
       "  (3): MaxPool2D(size=(3, 3), stride=(2, 2), padding=(0, 0), ceil_mode=False, global_pool=False, pool_type=max, layout=NCHW)\n",
       "  (4): Sequential(\n",
       "    (0): Conv2D(256 -> 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), Activation(relu))\n",
       "    (1): Conv2D(384 -> 384, kernel_size=(1, 1), stride=(1, 1), Activation(relu))\n",
       "    (2): Conv2D(384 -> 384, kernel_size=(1, 1), stride=(1, 1), Activation(relu))\n",
       "  )\n",
       "  (5): MaxPool2D(size=(3, 3), stride=(2, 2), padding=(0, 0), ceil_mode=False, global_pool=False, pool_type=max, layout=NCHW)\n",
       "  (6): Dropout(p = 0.5, axes=())\n",
       "  (7): Sequential(\n",
       "    (0): Conv2D(384 -> 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), Activation(relu))\n",
       "    (1): Conv2D(10 -> 10, kernel_size=(1, 1), stride=(1, 1), Activation(relu))\n",
       "    (2): Conv2D(10 -> 10, kernel_size=(1, 1), stride=(1, 1), Activation(relu))\n",
       "  )\n",
       "  (8): GlobalAvgPool2D(size=(1, 1), stride=(1, 1), padding=(0, 0), ceil_mode=True, global_pool=True, pool_type=avg, layout=NCHW)\n",
       "  (9): Flatten\n",
       ")"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on gpu(0)\n",
      "epoch 1, loss 1.7387, train acc 0.320, test acc 0.513, time 17.4 sec\n",
      "epoch 2, loss 0.7872, train acc 0.696, test acc 0.764, time 15.9 sec\n",
      "epoch 3, loss 0.5710, train acc 0.785, test acc 0.809, time 16.0 sec\n",
      "epoch 4, loss 0.5018, train acc 0.811, test acc 0.835, time 16.1 sec\n",
      "epoch 5, loss 0.4527, train acc 0.832, test acc 0.854, time 16.2 sec\n",
      "epoch 6, loss 0.4169, train acc 0.845, test acc 0.862, time 16.2 sec\n",
      "epoch 7, loss 0.3870, train acc 0.856, test acc 0.871, time 16.1 sec\n",
      "epoch 8, loss 0.3680, train acc 0.863, test acc 0.871, time 16.2 sec\n",
      "epoch 9, loss 0.3475, train acc 0.871, test acc 0.878, time 16.1 sec\n",
      "epoch 10, loss 0.3293, train acc 0.876, test acc 0.890, time 16.1 sec\n",
      "epoch 11, loss 0.3164, train acc 0.883, test acc 0.891, time 16.1 sec\n",
      "epoch 12, loss 0.3024, train acc 0.890, test acc 0.896, time 16.1 sec\n",
      "epoch 13, loss 0.2869, train acc 0.894, test acc 0.904, time 16.1 sec\n",
      "epoch 14, loss 0.2766, train acc 0.898, test acc 0.905, time 16.0 sec\n",
      "epoch 15, loss 0.2660, train acc 0.902, test acc 0.900, time 16.1 sec\n",
      "epoch 16, loss 0.2574, train acc 0.905, test acc 0.909, time 16.2 sec\n",
      "epoch 17, loss 0.2453, train acc 0.910, test acc 0.908, time 16.2 sec\n",
      "epoch 18, loss 0.2368, train acc 0.913, test acc 0.915, time 16.3 sec\n",
      "epoch 19, loss 0.2303, train acc 0.914, test acc 0.913, time 16.3 sec\n",
      "epoch 20, loss 0.2228, train acc 0.918, test acc 0.919, time 16.2 sec\n",
      "epoch 21, loss 0.2180, train acc 0.919, test acc 0.916, time 16.1 sec\n",
      "epoch 22, loss 0.2094, train acc 0.923, test acc 0.916, time 16.0 sec\n",
      "epoch 23, loss 0.2024, train acc 0.925, test acc 0.918, time 16.0 sec\n",
      "epoch 24, loss 0.1985, train acc 0.927, test acc 0.921, time 15.9 sec\n",
      "epoch 25, loss 0.1924, train acc 0.929, test acc 0.918, time 16.0 sec\n",
      "epoch 26, loss 0.1851, train acc 0.932, test acc 0.922, time 15.9 sec\n",
      "epoch 27, loss 0.1804, train acc 0.934, test acc 0.926, time 16.0 sec\n",
      "epoch 28, loss 0.1747, train acc 0.936, test acc 0.921, time 15.8 sec\n",
      "epoch 29, loss 0.1704, train acc 0.936, test acc 0.922, time 15.9 sec\n",
      "epoch 30, loss 0.1641, train acc 0.939, test acc 0.925, time 15.9 sec\n",
      "epoch 31, loss 0.1582, train acc 0.941, test acc 0.930, time 15.9 sec\n",
      "epoch 32, loss 0.1549, train acc 0.943, test acc 0.923, time 16.0 sec\n",
      "epoch 33, loss 0.1501, train acc 0.944, test acc 0.929, time 16.0 sec\n",
      "epoch 34, loss 0.1459, train acc 0.946, test acc 0.927, time 15.9 sec\n",
      "epoch 35, loss 0.1394, train acc 0.948, test acc 0.929, time 15.9 sec\n",
      "epoch 36, loss 0.1364, train acc 0.949, test acc 0.925, time 15.9 sec\n",
      "epoch 37, loss 0.1329, train acc 0.950, test acc 0.929, time 15.9 sec\n",
      "epoch 38, loss 0.1294, train acc 0.952, test acc 0.930, time 15.9 sec\n",
      "epoch 39, loss 0.1231, train acc 0.954, test acc 0.929, time 15.9 sec\n",
      "epoch 40, loss 0.1223, train acc 0.954, test acc 0.932, time 16.0 sec\n",
      "epoch 41, loss 0.1176, train acc 0.957, test acc 0.929, time 15.9 sec\n",
      "epoch 42, loss 0.1127, train acc 0.958, test acc 0.927, time 16.0 sec\n",
      "epoch 43, loss 0.1091, train acc 0.960, test acc 0.921, time 15.9 sec\n",
      "epoch 44, loss 0.1038, train acc 0.961, test acc 0.934, time 16.0 sec\n",
      "epoch 45, loss 0.0989, train acc 0.963, test acc 0.930, time 16.0 sec\n",
      "epoch 46, loss 0.0986, train acc 0.963, test acc 0.928, time 16.0 sec\n",
      "epoch 47, loss 0.0934, train acc 0.965, test acc 0.923, time 16.0 sec\n",
      "epoch 48, loss 0.0894, train acc 0.968, test acc 0.928, time 16.0 sec\n",
      "epoch 49, loss 0.0857, train acc 0.968, test acc 0.931, time 16.0 sec\n",
      "epoch 50, loss 0.0847, train acc 0.969, test acc 0.929, time 16.0 sec\n"
     ]
    }
   ],
   "source": [
    "lr, num_epochs, batch_size, ctx = 0.1, 50, 128, comm.try_gpu()\n",
    "net.initialize(force_reinit=True, ctx=ctx, init=init.Xavier())\n",
    "trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': lr})\n",
    "train_iter, test_iter = comm.load_data_fashion_mnist(batch_size, resize=128)\n",
    "comm.train_ch5(net, train_iter, test_iter, batch_size, trainer, ctx,\n",
    "              num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
